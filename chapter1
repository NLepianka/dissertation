\documentclass[12pt]{book}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{mathptmx}
\usepackage{epigraph}
\usepackage{setspace}
\usepackage{csquotes}
\usepackage[autocite=footnote]{biblatex-chicago}
\usepackage[english]{babel}
\addbibresource{sources.bib}

\setlength{\epigraphwidth}{5in}	
\setlength{\epigraphrule}{0pt}
\renewcommand{\epigraphflush}{center}
\renewcommand{\textflush}{flushleft}

\usepackage{etoolbox}
\makeatletter
\newlength\epitextskip
\pretocmd{\@epitext}{\ttfamily}{}{}
\makeatother

\sloppy
\renewcommand*{\postnotedelim}{\addperiod\space\midsentence}
\onehalfspacing

\begin{document}
	
\title{Yet of Books There Are A Plenty: The Bibliography of Literary Data}
\author{Nigel Lepianka}
\maketitle

\chapter{The Ideal of Aggregation}

\section{Data}

\textbf{OIAS Definition of Data:}
"a reinterpretable representation of information in a formalized manner suitable for communication, interpretation, or processing. Examples of data include a sequence of bits, a table of numbers, the characters on a page, the recording of sounds made by a person speaking, or a moon rock specimen" (Consultative Committee for Space Data Systems 1-9).
\\

\textbf{Definition of transmission is from Shilingsburg:}
"The reproduction of texts from one document to another. Transmission need not involve variation...Understanding textual transmission involves identifying textual alterations and all the circumstances influencing their production and reproduction" (175).
\\

\textbf{Kirschenbaum- .txtual condition}
"Access is thus duplication, duplication is preservation, and preservation is creation--and recreation," (16).
\\

\textbf{Johanna Drucker- \textit{Graphesis}}
"Captcha and Data and Interpretability"

\section{Bibliography}

\section{OCR, Reproductions, and Scholarship}
Optical character recognition, or OCR, represents the latest in a series of technological developments employed in the preservation, extension of accessibility, and, most importantly, the transmission of textual objects. A particularly robust definition of the process is defined by the IMPACT (Improving Access to Text) project, based in the EU: 
	\begin{displayquote}
	Optical Character Recognition (OCR) is the electronic translation of text-based images into machine-readable and editable text. Usually performed by software devices as part of a digitisation workflow, OCR works by performing a layout analysis of a digital image and breaking that image into smaller structural components to find zones of textual content. These zones will include the overall area of the page that features text. Within that zone the OCR software will identify individual lines of text, and within those lines will identify individual words and characters.

Many OCR software suites are available for many types of use, and each runs to slightly different standards and methodology. At its simplest, however, all OCR software follows the same basic principle: once the software engine has identified a single character, it runs that character’s properties through an internal classification of text fonts to find a match. It repeats the process for all characters within a word, and then runs that information through a dictionary of complete words to find a match. It extends this process through sentences, lines and text blocks until – ideally – all text in the image has been identified."\autocite{_impact_????}
 	\end{displayquote}

\noindent At its core, OCR represents a process of turning  the image of a page (or other discrete material unit of a text) into a readable text file. For humanistic endeavors, including the IMPACT project, but also others such as Texas A\&M's \textit{Early Modern OCR Project} (eMOP),\autocite{_emop_????} Leipzig University's \textit{Open Greek and Latin Project},\autocite{_open_????} or the Library of Congress' \textit{Chronicling America}\autocite{_chronicling_????} project for early American newspapers, OCR is wrapped in an ethos that exhorts ideals of access for research and teaching, and postulates an improved quality of life (of the mind) for scholars. This is obvious in the extension of the IMPACT acronym itself: Improving \textit{Access} for Texts. And while this is understood as the ethics for why OCR-based projects and their outputs (i.e. OCR'd texts) are necessary and a force for good in the scholarly world, I use the IMPACT definition because of its perceived pragmatism in explaining the process while at the same time echoing traditional textual scholarship discussions about the nature of text. 

Before I explain that statement further, however, it may be important to contextualize OCR and its specific place in textual discussions. There are new things OCR offers to bibliographical discourse, which will be explored, but OCR is also only a new technology in a series of technologies that have been used for the reproduction of text. Photographic reproduction, xerography, and microfilms have all been method or means that has been used to preserve, provide access to, and transmit images of pages since the late nineteenth century. These means of reproduction and transmission are familiar to those with extensive work in archives and libraries, as they are some of the primary ways ephemera and serial publications of previous centuries survived. G. Thomas Tanselle speaks in a detailed manner about these forms of reproduction, though primarily their uncertainty as witnesses to the text, though they may present a page image in such a manner as to be considered in absolute fidelity to the copy-text from which the image is derived.\footnote{Tanselle's exact definition of reproduction understood here is: "the product of any chemical or electrostatic process that aims to represent with exactness (though perhaps on an enlarged or diminished scale) not only the text of a given document but also the details of its presentation, insofar as they can be duplicated on a different surface." \autocite[26]{tanselle_reproductions_1989}} Tanselle's constant refrain for criticizing modes of reproduction points to the "inherent uncertainty" of all modes of transmission.\autocite[43]{tanselle_rationale_2011} According to Tanselle's rationale, photographic copies of images are not adequate replacements for the original printed texts; they are new documents themselves, as too many issues can affect the reproduction that would introduce variants to the work. Any form of page image, which excludes OCR due to the time of publishing, but is nonetheless relevant, can occlude documentary evidence that is noticeable only with the material text; the images can be manipulated, with emendations, annotations, or errors introduced by a human that would distinguish the photocopy from the printed copy-text (this issue, we can imagine, is exacerbated in a post-Photoshop world); and finally, the collection of page images that make up a whole text, may not have been in fact created from a single copy-text at all, but from multiples, should scans of pages from other copies be necessary in cases of damage or other effects that could obstruct reading. It is for all these reasons that Tanselle complains of any trust one would put into a reproduction of a text whilst believing it to be a faithful reproduction that unerringly transmits the original printed source to a photocopy, or more relevant here, a digital image.\footnote{Tanselle's work cited here is also a robust literature review and reading list of sources for the history of photographic reproduction. Though dated, the article itself can provide a thorough account of much early scholarship and discussions of these forms of technological textual reproduction. For Tanselle, however, many of the pieces are flawed because of their lack of discussion of the limitations and the overeagerness of scholars to discard their incredulity for these textual artifacts and to accept them as replacements for the originals. \autocite{tanselle_reproductions_1989}}

Tanselle illustrates a necessary divide between ways to approach a reproduction of a text--that between a belief that a reproduction can be a replacement, or even a viable stand-in for the printed text it is derived from, and the acceptance that textual reproductions are a new incarnation of the text that has sprung into existence, sans any authorial intent, with its own intricacies and forms of documentary evidence that bear upon the work. For Tanselle's reasons it's to admonish bibliographers who, to him, should know better than to entertain ideas that replacements for print editions of texts could be exist.\autocite[26]{tanselle_reproductions_1989} Tanselle's insistence that the facsimile reproductions of texts are new documents in their own rights is empowering for the processes that reproduce texts; Tanselle's line of thinking allows us to approach OCR'd versions of texts (as technological successors of textual reproduction) within the bounds of bibliographical theory insofar as we become able to claim an OCR'd reproduction of a text as a new edition of a text, designed and implemented as an editorial method of textual transmission in the creation of a new text for reading a work.\footnote{Tanselle is not alone in his thoughts. What Tanselle labels uncertainty about a text, Peter Shillingsburg has acknowledged the premise that part of accepting the nature of a text--whether or a book or a manuscript--is to accept that one instance of the text cannot exist in the same physical space as another instance of text. By extension, Matthew Kirschenbaum has acknowledged the same by extending the incongruence of textual objects to the digital realm; while maintaining their still material nature, Kirschenbaum argues that digital access to texts recalls unique physical operations on the part of the mechanism being used at the moment of access.\autocites[13]{shillingsburg_gutenberg_2006} {kirschenbaum_mechanisms_2008}}

Returning to the IMPACT statement, and its relevance to bibliography, the statement calls attention to the primary purposes of OCR: the creation of "machine-readable" and "editable" text. The output of OCR is defined primarily by its affordance to be interpreted and manipulated by a machine. The word \textit{editable} is important here because it implies that there is a necessity in the output of OCR to be changed, that it will need a human hand to intervene between the process of the OCR software reading an image, and the output being used for textual analysis, or whatever other function it may be accessed for. The primary audience, as related by its requirement to be "machine-readable," is that of the computer, and not the human reader. The human reader, of course, has the original .pdf or page image that they can always read themselves. I would additionally argue the Boolean logic of \textit{and} twists the two concepts of machine-readability and editing into co-dependent objects. Machine-readability is necessarily tied to data's ability to be reconfigured and modeled, either because the machine will read the text in ways that naturally deform it, or the methods of studying the text computationally as determined by the human scholar will demand non-standardized modes of reading information.\footnote{Stephen Ramsay argues this idea in \textit{Reading Machines}, wherein the possibility of computation interpretation for Ramsay is wholly determined by one's ability to do what is not possible in printed, material text: word frequencies, spatial or semantic changes to texts, and applicability of algorithms become the default premises of what digital hermeneutics is all about. He provides a glancing mention of textual scholarship and analytical bibliography in this discussion by foregrounding what he sees as a shared practice of "expos[ing] the bare facts of a text" between algorithmic criticism and the material disciplines, calling such work a "prelude" to the critical methods that follow this fact-determination process. \autocite[6]{stephen_ramsay_reading_2011}} 

Following the IMPACT statement to the second paragraph, the way this is accomplished is, interestingly, resonant with early modern methods of setting type, that is, with the focus of both OCR and hand-press methods of printing, where the individual letter is a discrete unit of the composition process. I compare this against the nineteenth-century methods of stereotype and electrotype printing, which render the printing process to the level of the page. Textual criticism and analytical bibliography have made the sites of their study in the hand press printing period the moment where human labor or decision-making has an effect on the production of the text; most commonly, this seems to revolve around the introduction and transmission of errors, but otherwise any sort of variant, whether or not they are labeled so harshly as an error, among editions or copies of texts will certainly fall under the scrutiny of the material disciplines. The importance of human labor and decision-making, however, is what I would like to focus on in regards to OCR, however. Bibliographical views on texts acknowledge that errors occur not because of faults in the machinery that printed the text, but because of mistakes made on the part of a human agent that is located at some point between and inclusive of the author and the reader.\footnote{This sentiment is rooted in D. F. McKenzie's \textit{Bibliography and the Sociology of Texts}, wherein his use of the word \textit{sociology} as it relates to bibliography hinges upon the recognition of the human element in the creation of texts; McKenzie relies on this in order to ensure that his description of sociology does not become as understood as rigidly empirical as the tradition of bibliography (that is, primarily, descriptive) he is trying to supplant. The human agency embedded in McKenzie's sociology of the text, then, is responsible for its ability to tap into "social motives": 
\begin{displayquote}
By dealing with the facts of transmission and the material evidence of reception, it can make discoveries as distinct from inventing meanings. In focussing on the primary object, the text as a recorded form, it defines our common point of departure for any historical or critical enterprise. By abandoning the notion of degressive bibliography and recording all subsequent versions, bibliography, simply by its own comprehensive logic, its indiscriminate inclusiveness, testifies to the fact that new readers of course make new texts, and that their new meanings are a function of their new forms. The claim then is no longer for their truth as one might seek to define that by an authorial intention, but for their testimony as defined by their historical use."
\end{displayquote}
\autocite[15,29]{mckenzie_bibliography_1999}}

What needs to be explained more thoroughly, then, is the way in which a process of transmission such as OCR does involve the same capabilities for one to make judgments, interpretations, and decisions about the object of the OCR process. In this instance, it is worth looking at the Early Modern OCR Project (eMOP), based out of Texas A\&M's Initiative for Digital Humanities, Media, and Culture. eMOP can be understood in this discussion, to be a way for early modern texts to become accessible to scholars beyond the ways they are traditionally. Like the IMPACT project's statement, the mission statement of eMOP can be read in one way as reinforcing the idea of what OCR is by means of the purpose of its output, but, as well, their mission statement introduces a sentiment more akin to the editorial statements of critically edited text.\footnote{The MLA provides a standard in its guidelines for scholarly editions that discusses these sorts of statements: \begin{displayquote}Scholarly editions generally include a statement, or series of statements, setting forth the history of the text and its physical forms, explaining how the edition has been constructed or represented, giving the rationale for decisions concerning construction and representation. This statement also typically describes or reports the authoritative or significant texts and discusses the verbal composition of the text—its punctuation, capitalization, and spelling—as well as, where appropriate, the layout, graphic elements, and physical appearance of the source material. Statements concerning the history and composition of the text often take the form of a single textual essay, but it is also possible to present this information in a more distributed manner.\end{displayquote}\autocite{_guidelines_????}} That is, their mission statement presents a scholarly viewpoint that guided their process:
\begin{displayquote}
The Early Modern OCR Project (Lead PI, Dr. Laura Mandell) is an effort, on the one hand, to make access to texts more transparent and, on the other, to preserve a literary cultural heritage. The printing process in the hand-press period (roughly 1475-1800), while systematized to a certain extent, nonetheless produced texts with fluctuating baselines, mixed fonts, and varied concentrations of ink (among many other variables). Combining these factors with the poor quality of the images in which many of these books have been preserved (in EEBO and, to a lesser extent, ECCO), creates a problem for Optical Character Recognition (OCR) software that is trying to translate the images of these pages into archiveable, mineable texts. By using innovative applications of OCR technology and crowd-sourced corrections, eMOP will solve this OCR problem.\autocite{_emop_????}
\end{displayquote}
\noindent This statement contextualizes the collection of texts, largely derived from the Early English Books Online (EEBO) and Eighteenth-Century Collections Online (ECCO) databases, with particular bibliographic features that had to be approached as conscious issues inherent to texts from the hand-press period. Just as with the IMPACT statement, eMOP presents a conscious consideration for the outputs of the OCR process, but where the two differ is in the acknowledged purpose of the process, and their individual projects' thoughts as to the scale of their work. In comparison to the IMPACT statement, which refers primarily to the individual text, and the individual components that make up a document in the OCR process, the eMOP project's approach to early modern texts considers a more broad view; an individual text is displaced in favor of the aggregate that makes up a "literary cultural heritage." The stated purposes of the outputs of the OCR'd texts corresponds to this larger view, wherein the OCR'd texts can be considered viable because they are "archiveable" and "mineable." Both terms specifically place the texts within a context that defines the individual artifacts by their position within a larger collection; first, the term \textit{archiveable} declares a necessity for an overarching structure that surrounds the items within the collection. Archives, are, in the minds of digital humanists and literary critics, collections of objects that are organized by a taxonomy that is both informed by and informs litery scholarship as to the relationship amongst the materials, rather than their individual values.\footnote{Kate Theimer describes the discrepancy between the digital humanist approach to archives and that of formally trained archivists. For Theimer the divide between conceptions of the archives hinges on matters of selection and authenticity. In my statement above, I account for the idea of selection, and how materials in a collection are organized that seems to scholars the most important detail of the collection. Theimer, however, notes that archivists are not in the business of selection, but instead in contextualization. Following Tanselle's belief that reproductions of a text are new documents and not replacements for the original text, we can understand the selection-based idea of archives as more fitting for materials that are not, in Tanselle's sense, part of any historical context that would give archivists cause to associate them with the original materials.\autocite{theimer_archives_2012}} In more direct words, texts found in the EEBO and ECCO collections are defined by their periodicity--they belong to a category that is innately literary, as described by Mandell.\footnote{Mandell points to early publications of poetry anthologies that organize literature according to historical and canonical interest and the idea of the "Romantic" period emerging at the same moment as the poetry it refers to.\autocite[83]{mandell_digitizing_2013}} The important shift that occurs here, though, and which is reflected in eMOP's ideological approach to OCR, is the idea that it is not the aesthetics that should inform periodization, but rather the medium, the history of the book and its production, that should define the means of collection and presentation, and in the case of eMOP, the actual methods that undergird the OCR process.\autocite[90]{mandell_digitizing_2013} 

The term \textit{mineable} carries the same connotation. The mining of texts (which refers to the text mining, or computational analysis of and algorithmic operations run on a collection of texts) also necessitates an aggregate. In cases in which smaller collections or individual texts may be concerned, what can be deemed "the aggregate" are the words, however, the most opportune uses of text mining, or "distant reading", look to collections comprised of whole documents. This returns to the purpose of OCR's outputs as understood by those involved with their production: if one of the primarily understood reasons for why a text will be used is that it's individual features will be enmeshed within the dominant patterns visible at scale, then the process to edit and represent these texts, must take in mind the dominant patterns of textual production. This is primarily Mandell's point in reinforcing the idea of the early modern period in the digital age via the means of production of texts over semantic or historical categories that have also defined periods: 

\begin{displayquote}
"...whether thinking about the difference between coterie print and mass print cultures, the or taking account of our current encounter with 'big data' and 'distant reading,' shoving the eighteenth century out of modernity and into early modernity suggests even more: that \textit{scale} matters most about medium as a system of production, circulation, and dissemination."\footnote{\autocite[90]{mandell_digitizing_2013} Empasis Mandell's}
\end{displayquote}

\noindent eMOP's statement puts Mandell's claim into a practice by primarily describing the objects of their work confined to the humanistic early modern period as coterminous with the bibliographic hand-press printing period, and by addressing the details this period in terms of its mechanical flaws: "fluctuating baselines, mixed fonts, and varied concentrations of ink (among many other variables." It is only through understanding these innately bibliographical processes that the texts can be prepared for future use. What Mandell and the eMOP statement both understand is that the semantic, aesthetic, and theoretical analysis of literature is predicated entirely on the material conditions of the text, which forms the foundation of its ability to be transmitted. This is a point bibliography understands well, but where OCR expands this idea is in its capacity to make the hand-press period a scholarly literary period in itself, as the mechanical process that created the texts in the collection become the overall category that defines the collection which will become subject to scholarly questions of political, aesthetic, and otherwise semantic exploration of language that may be considered by the scholars asking the questions to be independent from the processes of production. For bibliography and the material disciplines to adequately study the transmission of texts, they must account for the concept of scale as a capacity for texts to be amongst one another in a collection. This, to a degree, has already been considered by ennumerative bibliographers, a point which I will return to later, but is not wholly within their domain. 

\section{Non-Consumptive Data and the Layers of Transmission}

OCR is not necessarily an end to the process of textual reproduction in itself. Arguably, text files produced by the OCR process are really only the beginning of a trail that will involve far more radical reproductions and deformances of the text. The products of the OCR are meant to be manipulable to better serve the individual scholar's  needs; this is one of the chief affordances of the digital medium, and coincides with the exponential capability to reproduce myriad editions of a text from the base copy-text, though they may not necessarily be considered so, even by the scholar-editor creating them. For example, the beginning of any sort of text mining operation on a collection of 1000 early modern dramas will involve some degree of what is called pre-processing, which is understood to be a way of editing the data so as to be most useful and readable for the specific process. In the case of early modern plays, this may involve the removal of punctuation, stage directions, \textit{dramatis personae}, and the removal of names and "stop-words" (typically, common words which are deemed to have little semantic meaning, i.e., \textit{the}, \textit{an}, \textit{a}, etc.). The choices to modify the text files in this way is of course an editorial decision, and is not always the same in every instance. Modifications to the early modern plays during pre-processing can also include decisions to modernize the spelling, changing the metadata or adding new classifications, such as dramatic genre, to the texts, or in removing period-specific terms (i.e. \textit{spark}). 

These decisions reflect a certain stance taken at a specific moment about the texts, but more crucially, what should be understood about the pre-processing and any sort of modification to a text file derived from OCR, is that the operations performed on them are meant to be performed on the entire collections of texts, rather than individual texts themselves. The point of view of the one doing the modifications is macroscopic, attempting to understand several thousands or more texts at once, or rather, to make any massive number of texts comprehensible. Seen in this light, editorial decisions about a collection of texts are designed to make the collection of texts conform to a standard of presentation without regards to their individual qualities. 

This is clearly presented in the documentation for HathiTrust's \textit{Word Frequencies in English-Language Literature, 1700-1922} dataset.\autocite{ted_underwood_word_2015} This documentation covers many of the decisions made in creating the dataset, revealing the frame of mind needed for managing the presentation of a collection of approximately 178,000 texts. Of immediate interest here is the decision of format as presenting the texts as word frequency lists, rather than the words printed as they correspond to the material and original OCR copies. This has come to be known as non-consumptive data, a term, as described in the Amended Settlement Agreement for \textit{Author’s Guild v. Google Inc.} which refers to research “in which computational analysis is performed on one or more Books, but not research in which a researcher reads or displays substantial portions of a Book to understand the intellectual content presented within the Book [sic]”\footnote{\textit{Author’s Guild v. Google Inc.} (2011) was one of two major legal disagreements about the extent of fair use in the mass digitization of books. There was also \textit{Author’s Guild v. HathiTrust} (2014), which was an offshoot of the Author’s Guild’s suit against Google, as HathiTrust uses some of Google’s images and digitizations in its own library. Both cases settled in favor of the defendants, protecting both digital libraries’ use and dissemination of non-consumptive data under fair use.} The idea of non-consumptive research and data emerges as a workaround for institutions such as Google or HathiTrust to supply scholars with textual information without violating the laws of copyright. Literary scholars working with texts from the twentieth-century onward have to face the issue that digital forms of their texts are simply less available (and usually only illegally available) than the texts preferred by their colleagues working in the nineteenth-century and earlier. Though to keep scholars of pre-copyright eras from being too complacent, similar restrictive issues can arise when dealing with specific rare editions, manuscripts, or papers that are owned and proprietary to institutions. Non-consumptive data exists then to help scholars do their research whilst abiding by the U.S.’s current copyright laws, by providing all of the data found within the text, but not replicating the way that information is represented on a page.

For example, Table 1.1 presents the "beginning" of the non-consumptive version of \textit{Moby-Dick} drawn from the \textit{Word Frequencies} dataset:
\begin{table}[h!]
\centering
	{\begin{tabular}{ |c|c| }
			\hline
			, 18933 & ;	4075 \\
			the	14237 & that 3018  \\
			. 7141 & " 2779 \\
			of 6547 & his 2494 \\
			and 6326 & it 2466 \\
			a 4600 & i 2151 \\
			to 4549 & he 1836 \\
			in 4118 & but 1759 \\
			\hline
	\end{tabular}}
	\caption{The ``beginning" of \emph{Moby-Dick}}
	\label{table:1}	
\end{table}
\noindent It is, in fact, Moby-Dick, the 1892 edition to be specific, and a good portion of it too (54,191 words of more than 200,000 plus a good deal of punctuation thrown in).\footnote{For a facsimile of the edition from which this edition of \textit{Moby-Dick} was drawn from see \autocite{melville_moby_1892}} The right side of the cells depicts the tokens (the words, punctuation, or numbers) found in the text, and their frequency, the numbers on the left side of the cell. Altogether, the table displays the top sixteen most frequent tokens found in the specific edition of \textit{Moby-Dick}

Matthew Sag clarifies the definition when he explains the concept, preferring "non-expressive" over "non-consumptive":
\begin{displayquote}
In the world of books, a non-expressive use is any use that, while it may literately [sic] involve reproduction of the work, does not involve any human reading the digitized copy of the book. If the data extracted does not allow for the work to be reconstructed, there is no substitution of expressive value. Extracting factual information about a work in terms of its linguistic structure or the frequency of the occurrence of certain words, phrases, or grammatical features is a non-expressive use.\autocite[1525-6]{sag_orphan_2012}
\end{displayquote}
\noindent The non-consumptive book must be absent of human involvement; the original, material text from which the data is derived must only be viewable to one entity: the machine scanning the book and transferring the contents therein to the server in which the data will be stored.  What interests me most, though is the second sentence, wherein Sag insists that the ability to reconstruct the work is tied to the expressive value of the work. This reveals a certain paradox about non-consumptive data: that the text is devoid of large swathes of information, yet promises to be useful to researchers seeking new knowledge. The traditional organization of the book has been obliterated, distilled into a form that attempts to obfuscate the fact that an individual book exists and insists only on the presence of the written characters and not the expressive information derived from their order and structure as dictated by the material book.\footnote{ For Sag, “The frequency table itself is metadata, data about the work that is entirely independent of the expressive value of the work. True enough, the data relies on the underlying work, but it has no similarity to the work in terms of plot, structure, character (other than the names of characters) or theme. ” Sag reveals that his understanding, and perhaps that of the legal arguments of non-consumptive data do not recognize the connection between the material aspects of the book and its content; only the aesthetic and semantic aspects of plot, theme, and so on are considered as intellectual property, and so are officially considered in the legal argument. \autocite[1520]{sag_orphan_2012}} 

But books are resilient objects. Their material form and their linguistic content are not entirely independent of each other, and this is a point that book historians, bibliographers, and textual scholars should know very well. Even when a book has been put through a digital woodchipper in an effort to make the text as non-consumptive as possible, remnants of its existence, its format, and the idiosyncrasies of the book (i.e. its bibliographic codes) remain.\footnote{The term \textit{bibliograpic code} is pulled from Jerome McGann's \textit{Textual Condition} wherein he posits the idea that a physical work of literature is composed of both linguistic codes, but also bibliographic codes that refer to the font, page layout, and other physical features in the construction and presentation of a text that bibliography knows well. McGann additionally defends the idea that bibliographic codes are just as expressive as the linguistic codes literary scholars preoccupy themselves with. \autocite{mcgann_textual_1991}.} This is what I want to prioritize in this part of the discussion: my aim is to discuss the way in which we can understand the logic of this allegedly non-consumptive form of data, especially as it proliferates among digital libraries and repositories seeking to provide extended access to texts for scholars. I consider non-consumptive forms of texts to be valid editions of texts, albeit less human-readable editions, that can still contain expressive information if one only seeks to do the work of excavating the patterns from the bags of words one is handed when dealing with non-consumption.

Of course, I do not mean to combat the notion of non-consumption as being separate from the expressive characteristics of a text so as to dismantle the legal argument that has served to broaden the accessibility to proprietary works for scholars. Instead, as someone who sees a new form of textual representation emerging and proliferating, I feel it is the responsibility of bibliographers and textual scholars, both for the development of our own field and as a service to our colleagues, to consider the new form, and its relation to the history of the book and analyze the properties of the text as they are represented. In essence, by considering even the non-consumptive form of a work as an edition, we open up a level of bibliographic critique that pushes against our definitions, standards, and understanding of text. By observing the contents of something as oblique as a word frequency list, it is possible to discover traces and clues as to a text’s structure and form, giving us a sense of both the interplay between the formal properties of the book, and the interplay of the language and logic used to refer to or represent those properties.\footnote{To be clear, we should understand one of the primary affordances of non-consumptive data is to help distant reading scholarship, which relies on massive digitization of texts. Datasets like HathiTrust’s Word Frequencies are designed with the assumption that scholars will not necessarily only access a single text for their work, but the entire corpus or at least a sizable sample of the 178,381 items contained therein. This fact does not preclude my argument, but merely brings to our attention the intended audience of these editions of works. } To prove this point, we can dissect one these non-consumptive files, seeing if we can unearth bibliographic properties that gives us a clue as to what kind of book we may be looking at. 

To return to the example of the HathiTrust \textit{Word Frequency} dataset, we can understand a collection of word frequencies as an edited and curated collection (but perhaps not in the way most scholars are most familiar with), in which critical, editorial decisions were made. The files generated by HathiTrust used OCR, a method that attempts to recognize lexical characters in an image and translate those characters to text in order to turn their page image .pdfs of book files (i.e. what you see when you access a specific, public domain title in HathiTrust) into a text file accessible to programs like MS Word, Notepad, etc. Ryan Cordell, in agreement with my own views, explicitly connects automated methods of text creation with the historical processes of printing more readily understood by traditional scholars: "We might think of OCR as a species of compositor: prone to transcription errors, certainly, but nonetheless resetting the type of its proof texts into .txt or .xml files rather than printer’s frames."\footnote{\autocite{cordell_ryan_q_????}.  To further the point, Ted Underwood proves with the Word Frequency dataset that an English professor is equally capable of providing a critical eye to the development of a resource for literary study, regardless of the technological aspect of the resource’s development. Underwood provides guidelines for the process of correcting the common OCR errors (e.g. the long s found in early English texts, which is often OCR’d as an f) in the dataset dutifully addressing the ultimately human involvement in what is thought as a wholly mechanical process: "All these decisions involve judgment calls that could be made differently, so use this at your own risk. There is no warranty explicit or implicit, etc. Try running it on a sample set of files and see if you like the translation. If you would do things differently, feel free to fork the code and rules and change them." Underwood makes explicit the ultimately subjective and critical decisions involved in the creation of a large corpora of literary information. \autocite{underwood_tedunderwood/datamunging_????}} The connection between current methods of text representation and previously standard methods, Cordell claims, allows us to understand the twenty-first century’s innovations in printing as contiguous with the overarching book history narrative, rather than as a dichotomy separating the supposedly exacting human touch of historical printing methods from the cold, lifeless scan of a machine. Understanding the dichotomy of printing as human and digitization as automatic, to Cordell, necessarily requires that one "both overestimate the autonomy of human compositors in print shops and underestimate the role of computer scientists in OCR." \autocite{cordell_ryan_q_????} In the case of the HathiTrust dataset, the text files produced are born from physical books, which serve as the copy-texts for the .txt file, non-consumptive edition that results. The features contained within the text file are limited to only what the printed text contained or its own understanding of what the features are. Framed this way, an OCR-created text is little different from human-edited text; both reproducing a specific instance of a work informed by a previously existing text, and both subject to the introduction of transmission errors that are ultimately informed as well by the principal text. While the use of only seems constricting, it is, to me, actually a very helpful aspect of placing this discussion within the bibliographical concept of transmission history, but in also bringing back to the discussion the idea that the book’s, that is, the material book’s existence is going to influence the eventual digital output of an object like a non-consumptive .txt file. 

To explore what I mean by this it will be helpful to view a case study of study of a specific text that has been OCR'd from a material book, and taken from a scanned text to a highly ambiguated non-consumptive form. For this study, I will guide us through the 1838 London edition of Edgar Allan Poe's \textit{The Voyage of Arthur Gordon Pym}.\footnote{The facsimile of this text can be found on HathiTrust. See \autocite{poe_narrative_1838}} Firstly, let us examine the top sixteen most frequent tokens found in \textit{Pym} displayed in Figure 1.2:\footnote{I openly admit and recognize that I am performing an editorial act, manipulating the text and structuring information in such a way so as to force the reader to see the same patterns and draw the same conclusions as I am.}
\begin{table}[h!]
\centering
	{\begin{tabular}{ |c|c| }
			\hline		
,	5898 & was	943 \\
the	5179 & i	912 \\
of	2911 & we	823 \\
.	2499 & with	749 \\
and	2122 & as	704 \\
to	1955 & had	702 \\
in	1524 & that	686 \\
a	1511 & it	677 \\
			\hline
	\end{tabular}}
	\caption{The ``beginning" of \textit{Arthur Gordon Pym}}
	\label{table:2}	
\end{table}

\noindent There are a few things we can immediately assume from this, despite it seeming like very little useful information is supplied. Compare it to Figure 1.1; there is an obvious discrepancy between the relative frequency of terms in this text as compared to Moby-Dick. There are a little less than a third of the commas in this text, and a little more than a third of the \textit{the}s. Overall, there are just less tokens total in the top sixteen tokens of this text than in \textit{Moby-Dick}. From this information we can begin to form an assumption about about the physical text of \textit{Pym}: having a third of the tokens, a linguistic feature, suggests the text is physically going to be smaller in terms of volume than a copy of a work like \textit{Moby-Dick}. A lot of books are smaller than that, though, so we can move on, but it is important to note that there is bibliographic evidence present, even in the noise of words so commonly used that they may mean nothing out of context.

Something a little more unique and a bit more interesting is the information of Figure 1.3:
\begin{table}[h!]
\centering
	{\begin{tabular}{ |c|c| }
			\hline		
i.	2 & xiii	1\\
ii	2 & xiv.	1\\
iii	2 & xv.	1\\
iv.	2 & xvi.	1\\
v.	3 & xvii	1\\
vi.	1 & xviii	1\\
vii	1 & xix	1\\
viii 1 & xx	1\\
ix	1 & xxi.	1\\
x	2 & xxii	1\\
xi.	1 & xxiii	1\\
xii	1 & xxiv.	1\\
			\hline
	\end{tabular}}
	\caption{Roman numerals found in \textit{Arthur Gordon Pym}}
	\label{table:3}	
\end{table}

\noindent What is this exactly? Well, it is Roman numerals one through twenty-four; no other Roman numerals occur in this text. Roman numerals are good indicators of a text’s structure, because, within the English language we generally have only a few traditional uses for Roman numerals in a text. They can be used to indicate the pages of an introduction, can be used to represent chapters, or to represent dates, especially dates on front matter indicating the year of publication, or could represent items in an enumerated list. Since there are no roman numeral sequences representing a number after 1700, the beginning year of the dataset this text is taken from, we may safely assume the year of publication is not listed in Roman numerals on the front matter (more about dating in a moment). So we are left with one less option.

Now, if we consider the fact that every number between i and xxiv is represented in this text suspicious, then we may suggest these numerals are indicative of chapter headings.\footnote{We are, of course, assuming there are no mistakes in labeling the chapters in the original printed version of this text.} We can corroborate that, because the word \textit{chapter} appears in the text twenty-four times. Thus, we have learned two things: 1) This text has twenty-four chapters, but the more unconscious realization to bring forward is 2) that we know this text represents its chapter headings by stating “Chapter I”, “Chapter II”, “Chapter III”, and so forth.\footnote{There is still some mystery, though. We do not know if the format is in all capital letters or not, since the text file is pre-processed by HathiTrust to remove all upper case characters.} The logic used here is partially owed to Randall McLeod, who has helpfully pointed out that a text’s “structural redundancies are, crucially, both \textit{of} its text and \textit{about} it.”\footnote{\autocite[246]{randall_mcleod_published_under_random_clod_information_1991}. Emphasis McLeod's} Redundancy, or more broadly understood, the general patterns of information that can emerge from a text that point to the markings more related to their construction, rather than their content, as demonstrated by McLeod’s study, can explain why certain attributes of the text are present, and what their relationship is, once one can is able identify the possible patterns that could exist, such as the existence and labeling of chapters.

A few questions about Figure 1.3 may arise if we look closely at the discrepancies between different numerals, particularly about \textit{i}. Figure 1.2 also includes \textit{i}, and claims it appears 912 times, but Figure 1.3 only indicates it occurring twice. Figure 1.3 also includes a period after the \textit{i}. That means that the OCR recognizes \textit{i.} and \textit{i} as separate tokens, and not instances of the same word. It also means that the OCR is picking up an additional typographical feature of the text, since it does seem to notice more than a few times a period following a Roman numeral. There are a couple of possibilities here. First, the computer could be looking for another trace within the text that tells it whether the period is not at the close of a sentence, and that there is a difference between periods used for ending a sentence (which are said to occur 2499 times according to Figure 1.1) and those that are used to demonstrate some other logical representation. Second, there could be a typographical difference, including something like the size of the mark, between the periods used to follow the Roman numerals and others that occur within the body of the text that either confuses or informs the OCR that the period is marking something other than a sentence. This does allow us to refine our knowledge of the chapter headings, though, to suggest that the proper typesetting of the text included marking chapters as “Chapter I.” and so on, punctuating each announcement of a new chapter.

But there is still also the question of why there are two occurrences of \textit{i.}, \textit{ii}, \textit{iii}, \textit{iv.}, and \textit{x}, and three occurrences of \textit{v.} Our remaining two options for the numerals are that they represent introductory pages or list items. Luckily a search for the words that may indicate an opening section of a text (\textit{introduction}, \textit{prologue}, \textit{preface}, etc.) returns a result: \textit{preface.} (note the period) occurs twice, and\textit{ preface} (no period) once. This certainly suggests the possibility of introductory material, and the addition of the period to the end of two instances of the term, when we know that the chapters are written in such a way, would suggest some conformity with the standard section division labels. This would then make it necessary to explain why there are only three instances of preface, and not more to fit the extra numerals. Perhaps, there is a banner proclaiming the section of the text at the top of the page, but it only covers even or odd pages; this would make it more sensible for the \textit{iv.} and \textit{v.} numerals to have a place in the preface. That leaves one \textit{v.} and an \textit{x} still to place, but that unfortunately will have to remain a mystery.\footnote{ At least in the case of x, the lack of a period makes it easier to reconcile the fact that it may be a semantic part of the text that got lumped together with a typographical feature.}

I promised a discussion of dating, but since no Roman numerals were present that gave us a year past 1700, we have to rely on the presence of numbers that \textit{could} represent dates instead: 

\begin{table}[h!]
\centering
	{\begin{tabular}{ |c|c| }
			\hline		
1762	1 & 1811	2 \\
1767	1 & 1817	1 \\
1769	1 & 1820	1 \\
1772	2 & 1822	2 \\
1773	1 & 1823	1 \\
1774	1 & 1824	1 \\
1777	1 & 1825	1 \\
1779	1 & 1827	1 \\
1790	4 & 1828	1 \\
1791	1 & 1832	1 \\
1794	1 & 1837	1 \\
1803	1 & 1838.	2 \\
			\hline
	\end{tabular}}
	\caption{Year-like numbers found in \textit{Arthur Gordon Pym}}
	\label{table:4}	
\end{table}

Figure 1.4 shows all reasonable year-like numbers, and coincidentally they are all clustered around the end of the eighteenth-century and the beginning of the nineteenth, with 1790 being a particularly popular year for this text to mention. This makes our decisions actually fairly easy. If the front matter of the text is included in the OCR, which it is unless the page is completely missing from the physical book the information is drawn from, then one of these dates is the year of publication. Because I have already revealed that the copy-text of this non-consumptive file is the 1838 London edition of \textit{Pym}, the \textit{1838.}s are suspicious (and I would draw attention to the periods that conspicuously label those 1838s), but sans that information, a little common sense tells us that as the latest year in the list, it is also the most likely. It is of course possible that the text narrates a story that takes place in a year beyond its own publication, but for the late eighteenth-century or early nineteenth that would not be entirely common. 1838 is a promising choice, but more important than pinning down the exact year of publication, is that we have actually narrowed down the field of possible publication years, and are able to place this text within a certain context of book history that may help us figure out additional details. 

Particularly, knowing that the years of publication give us permission to make another assumption about that text: the size of the book. Figure 1.5 will allow me to explain:\footnote{The token \textit{a} is left out of the figure for what should be obvious reasons. It occurs 1511 times in this text, and since the other individual letters are not given any other way to make them distinct from semantic occurrences in the body of the text, we might assume that a couple of the \textit{a}s that could be in this figure are among the 1511.} 

\begin{table}[h!]
\centering
	{\begin{tabular}{ |c|c| }
			\hline		

b	3 & k	3 \\
c	4 & l	24 \\
d	2 & m	2 \\
e	2 & n	1 \\
f	3 & o	2 \\
g	3 & p	2 \\
h	3 & q	1 \\
j	3 &  \\
			\hline
	\end{tabular}}
	\caption{Roman numerals found in \textit{Arthur Gordon Pym}}
	\label{table:5}	
\end{table}

\noindent The first thing to know, is that when we look at all the numbers (disregarding the Roman numerals, and disregarding the years), the largest number present is 248. To save space, I won’t present it here, but most of the numbers between one and 248 occur at least once in the file. There is a good reason to suspect that seeming presence of sequential counting up to 248 is representing of the pagination being recorded by the OCR, and since the OCR was in fact page-level, we have no reason to suspect that the pages are not counting upwards. Remember, since this is a late eighteenth to early nineteenth-century book, still solidly in the world of moveable type, this book was likely printed on large sheets of paper folded into leaves. Now, 248 pages would be difficult to have if these were folded leaves, since these were either folded into two, four, or eight leaves. It is far more likely the text contains 256 pages, even if they are not all printed pages, as that would mean the book was more sensibly constructed by assembling thirty-two sheets bound into a quarto form (which would produce eight pages per sheet), or sixteen sheets bound into a octavo form (which would produce sixteen pages per sheet).

What does the information in Figure 1.5 represent then? The page signatures of the folded sheets. To remind those who know and initiate those are who may not, bookmaking before industrially produced paper (which eventually produced rolls and reams of paper) involved the use of large sheets that were folded to an appropriate size and bound. To signal to the bookbinder how and in what order the book was to be bound, there would be indicators printed at the bottom of what would be considered the first page of each leaf that indicated its place in the book’s intended order. There are many ways in which this was done, and the actual means of collation, the formal term for this system, varied by printer. An easy example formula is A1 $\rightarrow$ A2 $\rightarrow$ B1 $\rightarrow$ B2 $\rightarrow$ etc., which shows that the book bound by this formula has two leaves for each alphabetical letter, each being labeled as either one or two, and this follows until one has enough leaves to print all the information of the book, with usually some blank pages left over. 

If we believe at least a few of the tokens in Figure 1.5 to refer to page signatures, then we can form a hypothesis as to the book’s size. First, we know that both \textit{n} and \textit{q} appear once, which means that the text being assembled with alphabetical signatures without numbers is impossible. \textit{Q} is the seventeenth letter of the alphabet, meaning that if there was one leaf for each letter \textit{A} through \textit{Q} folded into a quarto size, the book would be 17 X 8 pages, or 136 pages long; too short for this text. If it were folded into an octavo size it would be 17 X 16, or 272 pages long; a bit high, but not out of the question. Given the available information of the figure, we also know that the maximum number of times an alphabetical symbol for page signature is two, since the tokens \textit{d} and \textit{e} occur twice, and the possibility that the book reaches 256 pages with only seven sheets for signatures \textit{b} and \textit{c} (occurring in this text three and four times respectively) is low. If the sheets did only get labeled to C (with possibility of up to 4), it would give us fifty-six pages in the quarto, and 112 in the octavo, meaning that leaves labeled with the A signature would need to make up the other 144-200 pages, and that would meaning have A signatures numbered 1-9 at the least (for the octavo), and 1-25 at the most (for the quarto). Again, this is \textit{technically} possible (we have 1511 instances of a to work with, I suppose), but common sense dictates it is likely not the case, as an excessive number of signatures of the same letter only increased the chances of binding the book incorrectly.  

Thus, the A1 $\rightarrow$ A2 $\rightarrow$ B1 $\rightarrow$ B2 $\rightarrow$ etc. formula given as an example earlier seems the most likely candidate. That would give us enough room to assume the letters A through M all appear twice, with the possibility of one sheet labeled N. But we can deduce further: with the aforementioned formula, we would have a maximum of twenty-seven leaves, which would translate in the quarto form to 216 pages and in the octavo form to 432 pages. Well, 216 is too low, so we cannot possibly have twenty-seven leaves in the quarto form, and cannot, in fact, have a quarto form at all since a minimum of thirty-two sheets would be required, as previously stated. That means this text file must have been generated from a book printed as an octavo, or about the size of a modern paperback.  

As stated at the beginning of this section, books are resilient objects. The point of this exercise, besides being a fun mental game to play and a mystery to solve, was to address the fact that the concepts bibliographers and textual scholars have clamored over for decades can be found to still be relevant when books are undergoing a transformation, and finding a new life in forms as unwieldy or daunting as that of non-consumptive data. As I have shown, the non-consumptive form of \textit{Pym} is still very much in conversation with its predecessor, taking the features of the 1838 Wiley and Putnam edition as its copy-text and in doing so, generating a file that is informed by that edition’s idiosyncrasies. Those who work closely with literature in the form of computational data, I suspect, have considered even momentarily that the origins of the data they work with still inflect, in some capacity, the data itself, regardless of how removed and different it seems from the printed object. 

What I wish to “make conscious,” to borrow McKenzie’s words, was that not words alone are not the only features of a text are visible in nonconsumptive form. In fact, divorced from their sequential order, the linguistic properties of a text can seem far more obtrusive to considering an individual text. The bibliographic features, however, can be sewn back together fairly easily by considering the relatively simple but structural patterns that books have and by making overt the ways texts announce properties about themselves. 

While it is unlikely one will ever curl up to read a non-consumptive edition of \textit{Moby-Dick} or \textit{Arthur Gordon Pym}, to consider that such an incarnation of those texts are valid in their own right and still a part of the still unfolding history of those works is not an absurd or even radical thought. Like any reprinting of a text, the information contained within the vessel that bears the title of a work, regardless of what form it may be in, still bears traces of its past life. Even when a text has been subjected to demands and processes that their creators could not have imagined, its physical essence is never truly lost; the book is still in there. 

\end{document}
